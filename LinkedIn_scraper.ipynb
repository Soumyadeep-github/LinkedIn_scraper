{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinkedIn_scraper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lLtkNLwa1n_"
      },
      "source": [
        "################# Import Statements #####################\n",
        "\n",
        "# !apt-get update\n",
        "# !apt install chromium-chromedriver\n",
        "# !cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "import csv\n",
        "import pandas as pd\n",
        "import requests\n",
        "from random import randint\n",
        "from time import sleep\n",
        "from bs4 import BeautifulSoup as bs\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys \n",
        "from scrapy.selector import Selector\n",
        "from selenium.webdriver.common.by import By\n",
        "from defaultlist import defaultlist\n",
        "from parsel import Selector\n",
        "from collections import OrderedDict\n",
        "import random\n",
        "from selenium.webdriver.support.ui import WebDriverWait \n",
        "\n",
        "from http_request_randomizer.requests.proxy.requestProxy import RequestProxy\n",
        "req_proxy = RequestProxy() #you may get different number of proxy when  you run this at each time\n",
        "proxies = req_proxy.get_proxy_list() #this will create proxy list\n",
        "\n",
        "\n",
        "################# Get the proxy(ies) that resolve LinkedIn in less than 5 seconds #####################\n",
        "def get_proxy(proxies=proxies):\n",
        "  #Get only the proxies with the ports 8080\n",
        "  port_80 = []\n",
        "  for i in proxies:\n",
        "      if '8080' in i.get_address():\n",
        "          port_80.append(i.get_address())\n",
        "\n",
        "  PROXY = []\n",
        "\n",
        "  #Get some ports that will resolve LinkedIn in less than 5 seconds\n",
        "  for i in port_80[randint(1,len(proxies))]:\n",
        "      proxy = {\"http\": i, \"https\": i}  \n",
        "      try:\n",
        "          resp = requests.get('https://www.linkedin.com', proxies=proxy, timeout=5)\n",
        "          PROXY.append(i)\n",
        "      except:\n",
        "          pass\n",
        "  return PROXY\n",
        "\n",
        "\n",
        "################ Using Webdriver to call LinkedIn #####################\n",
        "\n",
        "def linkedin_login(driver, email, password, PROXY_relevant):\n",
        "    \"\"\"Logs you into LinkedIn in the ChromeWebdriver.\"\"\"\n",
        "    webdriver.DesiredCapabilities.CHROME['proxy'] = {\n",
        "        \"httpProxy\": PROXY_relevant,\n",
        "        \"ftpProxy\": PROXY_relevant,\n",
        "        \"sslProxy\": PROXY_relevant,\n",
        "        \"proxyType\": \"MANUAL\"\n",
        "    }\n",
        "\n",
        "    url = 'https://www.linkedin.com'\n",
        "\n",
        "    driver.get(url)\n",
        "    username = driver.find_element_by_class_name('input__input')\n",
        "    sleep(3)\n",
        "    username.send_keys(email)\n",
        "    passkey = driver.find_element_by_id('session_password')\n",
        "    sleep(2)\n",
        "    passkey.send_keys(password)\n",
        "    log_in_button = driver.find_element_by_xpath('//*[@type=\"submit\"]')\n",
        "    log_in_button.click()\n",
        "\n",
        "\n",
        "email = '<enter your email-id>'\n",
        "password = '<enter your password>'\n",
        "\n",
        "# Download Chrome driver from here  https://chromedriver.chromium.org/downloads and then provide the location where the webdriver is being stored.\n",
        "driver = webdriver.Chrome('/usr/bin/chromedriver')\n",
        "\n",
        "PROXY_relevant = get_proxy()[-1]\n",
        "\n",
        "linkedin_login(driver, email, password, PROXY_relevant)\n",
        "# This block will simply open and log you into LinkedIn.\n",
        "# Once done, you have to head over to Google to search for the profiles for which you want the data.\n",
        "\n",
        "################ Get LinkedIn profiles from Google #####################\n",
        "\n",
        "sleep(5)\n",
        "#In a new page, in Selenium webdriver open Google.\n",
        "driver.execute_script(\"window.open('https://www.google.com/','_self')\")\n",
        "\n",
        "sleep(5)\n",
        "# locate search form by_name\n",
        "search_query = driver.find_element_by_name('q')\n",
        "\n",
        "if driver.find_element_by_name('q').get_attribute('value'):\n",
        "    driver.find_element_by_name('q').clear()\n",
        "\n",
        "search_term = input('Please enter search term: ')\n",
        "location = input('Please enter location: ')\n",
        "\n",
        "#Clear if the search box already contains any string\n",
        "# search_query.send_keys(Keys.CLEAR)\n",
        "\n",
        "sleep(3)\n",
        "\n",
        "if location == \"\" or \" \" or None:\n",
        "    # send_keys() to simulate the search text key strokes\n",
        "    search_query.send_keys(f'site:linkedin.com/in/ AND {search_term}')\n",
        "else:\n",
        "    search_query.send_keys(f'site:linkedin.com/in/ AND {search_term} AND {location}')\n",
        "                           \n",
        "# .send_keys() to simulate the return/enter key \n",
        "search_query.send_keys(Keys.RETURN)\n",
        "\n",
        "################ Get profile data from their respective pages #####################\n",
        "def extract_all_profile_data(driver, linkedin_urls):\n",
        "    \"\"\"Extract data from each page using xpaths.\"\"\"\n",
        "    # Create a dictionary of lists to store the scarped data.\n",
        "    data = {}\n",
        "    url = defaultlist(lambda: 'empty')\n",
        "    name = defaultlist(lambda: 'empty')\n",
        "    job_title = defaultlist(lambda: 'empty')\n",
        "    company = defaultlist(lambda: 'empty')\n",
        "    education = defaultlist(lambda: 'empty')\n",
        "    designation = defaultlist(lambda: 'empty')\n",
        "    location = defaultlist(lambda: 'empty')\n",
        "    qualification = defaultlist(lambda: 'empty')\n",
        "\n",
        "    # Using Selectors to scrape data. \n",
        "    # Each loop iteration goes through one page and extracts the data in the specified locations (location specifoied by the xpaths).\n",
        "    for page_num, page_url in enumerate(linkedin_urls):\n",
        "\n",
        "        if page_num == randint(0, len(linkedin_urls)):\n",
        "            sleep(randint(10, 30))\n",
        "        sleep(randint(1, 5))\n",
        "        url.append(page_url)\n",
        "        driver.get(page_url)\n",
        "        text = driver.page_source\n",
        "\n",
        "        sleep(randint(5, 20))\n",
        "        try:\n",
        "            name.append(Selector(text=text).xpath(\n",
        "                '//*[@class=\"inline t-24 t-black t-normal break-words\"]/text()').get().strip())\n",
        "        except:\n",
        "            name.append('No info')\n",
        "\n",
        "        try:\n",
        "            job_title.append(\n",
        "                Selector(text=text).xpath('//*[@class=\"mt1 t-18 t-black t-normal break-words\"]/text()').get().strip())\n",
        "        except:\n",
        "            job_title.append('No info')\n",
        "\n",
        "        try:\n",
        "            if Selector(text=text).xpath(\n",
        "                    '//*[@class=\"text-align-left ml2 t-14 t-black t-bold full-width lt-line-clamp lt-line-clamp--multi-line ember-view\"]/text()').get().strip() is None:\n",
        "                company.append(Selector(text=text).xpath(\n",
        "                    '//section/div[1]/section/ul/li[1]/section//div[2]/h3[@class=\"t-16 t-black t-bold\"]/span[2]/text()').get().strip())\n",
        "            else:\n",
        "                company.append(Selector(text=text).xpath(\n",
        "                    '//*[@class=\"text-align-left ml2 t-14 t-black t-bold full-width lt-line-clamp lt-line-clamp--multi-line ember-view\"]/text()').get().strip())\n",
        "        #         company.append(Selector(text=text).xpath('//*[@class=\"text-align-left ml2 t-14 t-black t-bold full-width lt-line-clamp lt-line-clamp--multi-line ember-view\"]/text()').get().strip())\n",
        "        except:\n",
        "            company.append('No info')\n",
        "\n",
        "        try:\n",
        "            if Selector(text=text).xpath(\n",
        "                    '//section/div[2]/div[2]/div[1]/h2[@class=\"mt1 t-18 t-black t-normal break-words\"]/text()').get().strip() is None:\n",
        "                designation.append(Selector(text=text).xpath(\n",
        "                    '//section/div[1]/section/ul/li[1]/section/ul/li[1]//h3[@class=\"t-14 t-black t-bold\"]/span[2]/text()').get().strip())\n",
        "            else:\n",
        "                designation.append(Selector(text=text).xpath(\n",
        "                    '//section/div[2]/div[2]/div[1]/h2[@class=\"mt1 t-18 t-black t-normal break-words\"]/text()').get().strip())\n",
        "        except:\n",
        "            designation.append('No info')\n",
        "\n",
        "        sleep(randint(2, 5))\n",
        "        try:\n",
        "            if Selector(text=text).xpath(\n",
        "                    '//section/div[2]/div[2]/div[1]//li[@class=\"t-16 t-black t-normal inline-block\"]/text()').get().strip() is None:\n",
        "                location.append(Selector(text=text).xpath(\n",
        "                    '//section/div[1]/section/ul/li[1]/section/ul/li[1]//h3[@class=\"t-14 t-black t-bold\"]/span[2]/text()').get().strip())\n",
        "            else:\n",
        "                location.append(Selector(text=text).xpath(\n",
        "                    '//section/div[2]/div[2]/div[1]//li[@class=\"t-16 t-black t-normal inline-block\"]/text()').get().strip())\n",
        "        except:\n",
        "            location.append('No info')\n",
        "\n",
        "        try:\n",
        "            #             if Selector(text=text).xpath('//section/div[2]/section/ul/li[1]//a/div[2]/div/h3[@class=\"pv-entity__school-name t-16 t-black t-bold\"]/text()').get().strip() == None or '':\n",
        "            education.append(Selector(text=text).xpath(\n",
        "                '//h3[@class=\"pv-entity__school-name t-16 t-black t-bold\"]/text()').get().strip())\n",
        "        #             else:\n",
        "        #             education.append(Selector(text=text).xpath('//section/div[2]/section/ul/li[1]//a/div[2]/div/h3[@class=\"pv-entity__school-name t-16 t-black t-bold\"]/text()').get().strip())\n",
        "        except:\n",
        "            education.append('No info')\n",
        "\n",
        "        try:\n",
        "            #             if Selector(text=text).xpath('//section/div[2]/section/ul/li[1]//a/div[2]/div/p[2]/span[@class=\"pv-entity__comma-item\"]/text()').get().strip() == None or '':\n",
        "            qualification.append(\n",
        "                Selector(text=text).xpath('//div//span[2][@class=\"pv-entity__comma-item\"]/text()').get().strip())\n",
        "        #             else:\n",
        "        #             qualification.append(Selector(text=text).xpath('//section/div[2]/section/ul/li[1]//a/div[2]/div/p[2]/span[@class=\"pv-entity__comma-item\"]/text()').get().strip())  \n",
        "        except:\n",
        "            qualification.append('No info')\n",
        "\n",
        "        sleep(randint(2, 4))\n",
        "\n",
        "    data['URL'] = url\n",
        "    data['Name'] = name\n",
        "    data['Job title'] = job_title\n",
        "    data['Company'] = company\n",
        "    data['Designation'] = designation\n",
        "    data['Location'] = location\n",
        "    data['Education'] = education\n",
        "    data['Qualification'] = qualification\n",
        "\n",
        "    #Data contains the ultimate data from all pages.\n",
        "    return data\n",
        "\n",
        "\n",
        "################ Save the data into CSV files #####################\n",
        "\n",
        "Users = pd.DataFrame.from_dict(extract_all_profile_data(driver,linkedin_urls))\n",
        "path = '/home/usr/Desktop/CSV/' #Change this path according to your file structure.\n",
        "Users.to_csv(path_or_buf = path + 'linkedin_profile_data.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}